{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "<a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">Principal Component Analysis</a> is a technique to reduce the dimension of training data to mitigate the <a href=\"https://en.wikipedia.org/wiki/Curse_of_dimensionality\">curse of dimensionality</a>\n",
    "\n",
    "The idea is to find a linear transformation of the original dataset to a smaller dimension dataset the preserves most of the data variance. The algorithms find the dimension of highest variance, then (rersively) project the data to the orthogonal space that is left and find the next highest dimension of variance, and keeps going until some stopping point (typically number of dimensions of percent of varianance). Given a set of training features X, this is equivalent to first creating the covariant matrix C (by subtract the mean of each feature from X, and computing X.T * X), and then using <a href=\"https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix\">eigendecomposition</a> (C = Q D Q.T, where D is diagonal sorted by eigenvalue, Q is a column matrix of eigenvalues), and projecting the features in X to the first \"k\" eigenvalues before further training:\n",
    "<center><img src=\"images/pca.jpg\" style=\"width:500px;height:250;\"></center>\n",
    "\n",
    "Note that this is quite imperfect, because:\n",
    "* PCA removes dimensions based purely on their variance, and independant of their impact on the predictor variable. So low-variance features that are highly correlated to the target variable being predicted may be removed.\n",
    "* It can be hard to interpret PCA features, as they are linear combinations of existing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on original data with 10 features: 0.00\n",
      "MSE on PCA-reduced data with 9 features: 0.98\n",
      "MSE on normalized dataset with 10 features: 0.00\n",
      "MSE on PCA-reduced data with 5 features: 0.50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "def pca(X, dim=2):\n",
    "    \"\"\" Perform PCA on X and return the reduced data.\n",
    "    :param X: data matrix\n",
    "    :param dim: dimension of the reduced data\n",
    "    :return: reduced data matrix, percent of variance explained by each component \"\"\"\n",
    "    pca = PCA(n_components=dim)\n",
    "    x = pca.fit_transform(X)\n",
    "    return x, pca.explained_variance_ratio_\n",
    "\n",
    "def pca_correlation_issue():\n",
    "    \"\"\" This shows the issue with PCA removing features that are highly correlated with the output, just because they have lower variance.\"\"\"\n",
    "    # Generate data, 100 samples, 10 features, one of the features matches the desired prediction perectectly, but has high variance\n",
    "    n_samples = 100\n",
    "    n_features = 10\n",
    "    y = np.random.randn(n_samples, 1)\n",
    "    x = np.random.randn(n_samples, n_features - 1) * 1000\n",
    "    x = np.hstack((x, y))\n",
    "\n",
    "    # That data gives a great model (MSE = 0), since it uses the low variance feature to predict the output\n",
    "    from sklearn import linear_model\n",
    "    model = linear_model.LinearRegression()\n",
    "    model.fit(x, y)\n",
    "    print (f\"MSE on original data with {x.shape[1]} features: {np.mean((model.predict(x) - y) ** 2):.2f}\")\n",
    "\n",
    "    # Now let's try PCA\n",
    "    x, _ = pca(x, dim=x.shape[1] - 1)\n",
    "    model.fit(x, y)\n",
    "    print (f\"MSE on PCA-reduced data with {x.shape[1]} features: {np.mean((model.predict(x) - y) ** 2):.2f}\")\n",
    "\n",
    "    # Even if all the features are initially scaled to have the same variance, PCA will still perform worse by partially removing the feature that is highly correlated with the output\n",
    "    x = np.random.randn(n_samples, n_features - 1)\n",
    "    x = np.hstack((x, y))\n",
    "    x = x / np.std(x, axis=0)\n",
    "    model.fit(x, y)\n",
    "    print (f\"MSE on normalized dataset with {x.shape[1]} features: {np.mean((model.predict(x) - y) ** 2):.2f}\")\n",
    "    x, _ = pca(x, dim=x.shape[1]//2)\n",
    "    model.fit(x, y)\n",
    "    print (f\"MSE on PCA-reduced data with {x.shape[1]} features: {np.mean((model.predict(x) - y) ** 2):.2f}\")\n",
    "\n",
    "pca_correlation_issue()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
